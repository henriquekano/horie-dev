<HTML>
<HEAD>
<META name=vsisbn content="0471503363">
<META name=vstitle content="Art of Computer Systems Performance Analysis Techniques For Experimental Design Measurements Simulation And Modeling">
<META name=vsauthor content="Raj Jain">
<META name=vsimprint content="Wiley Computer Publishing">
<META name=vspublisher content="John Wiley & Sons, Inc.">
<META name=vspubdate content="05/01/91">
<META name=vscategory content="Web and Software Development: Software Engineering: Simulation and Modeling">







<TITLE>The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling:Other Regression Models</TITLE>

<!-- HEADER -->

<STYLE type="text/css"> 
 <!--
 A:hover  {
 	color : Red;
 }
 -->
</STYLE>

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">

<!--ISBN=0471503363//-->
<!--TITLE=The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling//-->
<!--AUTHOR=Raj Jain//-->
<!--PUBLISHER=Wiley Computer Publishing//-->
<!--CHAPTER=15//-->
<!--PAGES=266-269//-->
<!--UNASSIGNED1//-->
<!--UNASSIGNED2//-->

<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="15-07.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="15-09.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>
<P><BR></P>
<H3><A NAME="Heading9"></A><FONT COLOR="#000077">15.6 COMMON MISTAKES IN REGRESSION</FONT></H3>
<P>The following mistakes are common in regression analysis and often lead to misleading conclusions.
</P>
<DL>
<DD><B>1.</B>&nbsp;&nbsp;<I>Not Verifying That the Relationship Is Linear</I>: If the scatter diagram shows a highly nonlinear relationship or, if based on other considerations, it is known that the relationship is nonlinear, a curvilinear relationship should be used. A linear regression for variables that are nonlinearly related may lead to the misleading conclusion that they are not related, for example, if the regression parameters are zero.
<DD><B>2.</B>&nbsp;&nbsp;<I>Relying on Automated Results without Visual Verification</I>: A visual check of the scatter diagram is an important step in regression analysis. Unfortunately, this step requires manual intervention and so it is often skipped. The analyst may use a measurement package, which automatically fits a regression model and presents the parameters. This may lead to misleading conclusions that could have been easily avoided by a visual check of the scatter diagram. For example, Figure 15.5 shows several scatter diagrams that will easily produce a regression with a high coefficient of determination but the model may not represent the system behavior. A quick look at the scatter diagram would have helped avoid the problem in each case.
<DD><B>3.</B>&nbsp;&nbsp;<I>Attaching Importance to Numerical Values of Regression Parameters</I>: The absolute values of the regression parameters depend upon the dimensions of the corresponding predictor variable. For example, consider the following hypothetical example of predicting CPU time as a function of the number of disk I/O&#146;s and the memory size of a program:
<P ALIGN="CENTER">CPU time in seconds = 0.01(number of disk I/O&#146;s)
<P ALIGN="CENTER">&#43; 0.001(memory size in kilobytes)
<BR>Here, a careless analyst may conclude that the parameter 0.001 is too small, and therefore, the memory size can be ignored as compared to the number of disk I/O&#146;s. This is wrong, since the same relationship could have been written as either
</DL>
<P ALIGN="CENTER">CPU time in milliseconds 10(number of disk I/O&#146;s)
</P>
<P ALIGN="CENTER">&#43; 1(memory size in kilobytes)</P>
<DL>
<BR>or
<P ALIGN="CENTER">CPU time in seconds = 0.01(number of disk I/O&#146;s)
<P ALIGN="CENTER">&#43; 1(Memory size in bytes)
</DL>
<DL>
<BR>In either case, the analyst may not consider that the regression parameter value of I is too small. The lesson is that the absolute value of the regression parameter depends upon the units in which it is meas=4 and two parameters measured in different units cannot be compared. The right way to compare the significance of a regression parameter is by its confidence interval.
<P><A NAME="Fig5"></A><A HREF="javascript:displayWindow('images/15-05.jpg',437,420 )"><IMG SRC="images/15-05t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/15-05.jpg',437,420)"><FONT COLOR="#000077"><B>FIGURE 15.5</B></FONT></A>&nbsp;&nbsp;Examples of data that may give high coefficient of determination, but the linear model obtained may not represent the system correctly.</P>
<DD><B>4.</B>&nbsp;&nbsp;<I>Not Specifying Confidence Intervals for the Regression Parameters</I>: Most analysts realize that the experiments need to be repeated, since observations are random variables. However, many analysts do not realize that all computations from a sample of <I>n</I> observations am also random. Therefore, a confidence interval should be specified that indicates the variability of the computation.
<DD><B>5.</B>&nbsp;&nbsp;<I>Not Specifying the Coefficient of Determination</I>: Least-squares estimation is a well-known technique. Sometimes analysts compute the regression parameters, but they do not specify the coefficient of determination <I>R</I><SUP><SMALL>2</SMALL></SUP>. Without <I>R</I><SUP><SMALL>2</SMALL></SUP>, it is difficult to deduce the goodness of the regression. An <I>F</I>-test is used even more rarely. However, if <I>R</I><SUP><SMALL>2</SMALL></SUP> is small, often we may not want to use the regression results anyway, and the <I>F</I>-test is not necessary.
<DD><B>6.</B>&nbsp;&nbsp;<I>Confusing the Coefficient of Determination and the Coefficient of Correlation</I>: The coefficient of correlation is denoted by <I>R</I> and the coefficient of determination by <I>R</I><SUP><SMALL>2</SMALL></SUP>. The coefficient of determination gives the percentage of explained variance; the coefficient of correlation does not. For example, if the coefficient of correlation is 0.8 (<I>R</I> = 0.8, <I>R</I><SUP><SMALL>2</SMALL></SUP> = 0.64), the regression will explain only 64% of the variance and not 80%.
<DD><B>7.</B>&nbsp;&nbsp;<I>Using Highly Correlated Variables as Predictor Variables</I>: Analysts often start a multilinear regression with as many predictor variables as possible. This may lead to severe multicollinearity problems. The correlations between predictor variables should be computed. Both variables of a correlated pair should be included in the regression only if this leads to a considerable increase in the significance of the regression.
<DD><B>8.</B>&nbsp;&nbsp;<I>Using Regression to Predict Far Beyond the Measured Range</I>: A system may behave differently in different operating ranges. A regression relationship based on measurements in one operating range may not apply far outside the range. In particular, the statistical confidence decreases as we move outside the measured range. Inexperienced analysts, who may not be aware of this, often use small workloads or systems for measurement and then use the regression to predict the behavior for large workloads or systems. In such cases, it is necessary to verify that the relationship will apply beyond the measured range. Also, the predictions should be specified along with their confidence intervals so that the decision makers can decide whether they are willing to take the risk caused by inaccuracies of the predictions.
<DD><B>9.</B>&nbsp;&nbsp;<I>Using Too Many Predictor Variables</I>: A common misconception among inexperienced analysts is that using more predictor variables increases the accuracy of predictions. We have already seen in Section 15.1.2 that adding a predictor variable that is correlated with others may decrease the statistical validity of the regression.
<BR>Given a set of <I>k</I> measured predictor variables, there are 2<SUP><SMALL><I>k</I></SMALL></SUP> &#150; 1 subsets that can be used. For example, given two variables <I>A</I> and <I>B</I>, we can try <I>A</I> only, <I>B</I> only, or both <I>A</I> and <I>B</I>. One can try all possible subsets and sort them in a decreasing order by the coefficient of determination <I>R</I><SUP><SMALL>2</SMALL></SUP>. Of course, the subset giving the maximum <I>R</I><SUP><SMALL>2</SMALL></SUP> is the <I>best</I> subset. But other subsets that are close may be used instead for practical or engineering reasons. For example, if the second best has only one variable compared to five in the best, the second best may be the preferred model.
<DD><B>10.</B>&nbsp;&nbsp;<I>Measuring Only a Small Subset of the Complete Range of Operation</I>: The measurements should cover as much of the typical operating range of a system as possible. For example, consider a computer system that supports up to 100 users. It is difficult to set up an experiment with a large number of users, and so the analysts will typically set up experiments with a small number of users, say, 10 or 20. Measurements in a limited range may result in a model that is quite different from that of the model that applies to the whole range. Figure 15.6 shows a hypothetical sample where the regression line that best fits the limited measured range is quite different from what would have been obtained if the whole range had been measured.
<P><A NAME="Fig6"></A><A HREF="javascript:displayWindow('images/15-06.jpg',437,420 )"><IMG SRC="images/15-06t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/15-06.jpg',437,420)"><FONT COLOR="#000077"><B>FIGURE 15.6</B></FONT></A>&nbsp;&nbsp;Measuring over only a portion of the range of operation may produce a model that does not apply to the whole range.</P>
<DD><B>11.</B>&nbsp;&nbsp;<I>Assuming that a Good Predictor Variable Is Also a Good Control Variable</I>: If two variables are highly correlated, one can be used to predict the other with a high precision. This is true even if one variable does not affect or control the other variable. A good regression model can be used to predict the performance, but if the goal of the study is to find what changes in the system will improve its performance, a regression model will be helpful only if the predictor variable is also a control variable. For example, a regression model of the disk I/O versus CPU time can be used to predict the number of disk I/O&#146;s for a program given its CPU time. However, reducing the CPU time by installing a faster CPU will not reduce the number of disk I/O&#146;s. In other words, CPU time is a predictor but not a controller of the number of disk I/O&#146;s.
<BR>If two variables <I>w</I> and <I>y</I> are both controlled by a third variable <I>x</I>, <I>w</I> and <I>y</I> may be highly correlated and would be good predictors for each other. The prediction works both ways: <I>w</I> can be used to predict <I>y</I> and vice versa. The control often works only one way: <I>x</I> controls <I>y</I> but <I>y</I> may not control <I>x</I>.
</DL>
<P><BR></P>
<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="15-07.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="15-09.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>

<hr width="90%" size="1" noshade>
<div align="center">
<font face="Verdana,sans-serif" size="1">Copyright &copy; <a href="/reference/wiley00001.html">John Wiley &amp; Sons, Inc.</a></font>
</div>
<!-- all of the reference materials (books) have the footer and subfoot reveresed -->
<!-- reference_subfoot = footer -->
<!-- reference_footer = subfoot -->

</BODY>
</HTML>

<!-- END FOOTER -->

