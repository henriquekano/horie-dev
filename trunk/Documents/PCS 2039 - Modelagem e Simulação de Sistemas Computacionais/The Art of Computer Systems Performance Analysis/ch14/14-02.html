<HTML>
<HEAD>
<META name=vsisbn content="0471503363">
<META name=vstitle content="Art of Computer Systems Performance Analysis Techniques For Experimental Design Measurements Simulation And Modeling">
<META name=vsauthor content="Raj Jain">
<META name=vsimprint content="Wiley Computer Publishing">
<META name=vspublisher content="John Wiley & Sons, Inc.">
<META name=vspubdate content="05/01/91">
<META name=vscategory content="Web and Software Development: Software Engineering: Simulation and Modeling">







<TITLE>The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling:Simple Linear Regression Models</TITLE>

<!-- HEADER -->

<STYLE type="text/css"> 
 <!--
 A:hover  {
 	color : Red;
 }
 -->
</STYLE>

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">

<!--ISBN=0471503363//-->
<!--TITLE=The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling//-->
<!--AUTHOR=Raj Jain//-->
<!--PUBLISHER=Wiley Computer Publishing//-->
<!--CHAPTER=14//-->
<!--PAGES=225-229//-->
<!--UNASSIGNED1//-->
<!--UNASSIGNED2//-->

<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="14-01.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="14-03.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>
<P><BR></P>
<P>A derivation of the expressions for regression parameters now follows.
</P>
<P><B>Derivation 14.1</B> The error in the <I>i</I>th observation is</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-09d.jpg"></P>
</IMGD></P>
<P>For a sample of <I>n</I> observations, the mean error is</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-10d.jpg"></P>
</IMGD></P>
<P>Setting the mean error to zero, we obtain</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-11d.jpg"></P>
</IMGD></P>
<P>Substituting <I>b</I><SUB><SMALL>0</SMALL></SUB> in the error expression, we get</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-12d.jpg"></P>
</IMGD></P>
<P>The sum of squared errors is</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-13d.jpg"></P>
</IMGD></P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-14d.jpg"></P>
</IMGD></P>
<P>The value of <I>b</I><SUB><SMALL>1</SMALL></SUB>, which gives the minimum SSE, can be obtained by differentiating this equation with respect to <I>b</I><SUB><SMALL>1</SMALL></SUB> and equating the result to zero:</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-15d.jpg"></P>
</IMGD></P>
<P>That is,</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-16d.jpg"></P>
</IMGD></P>
<P>This is Equation (14.1) presented earlier.</P>
</BLOCKQUOTE><H3><A NAME="Heading4"></A><FONT COLOR="#000077">14.3 ALLOCATION OF VARIATION</FONT></H3>
<P>The purpose of a model is to be able to predict the response with minimum variability. Without a regression model, one can use the mean response as the predicted value for all values of the predictor variables. The errors in this case would be larger than those with the regression model. In fact, in this case, the error variance would be equal to the variance of the response, since
</P>
<DL>
<DD>Error = <I>&#917;</I><SUB><SMALL>i</SMALL></SUB> = observed response &#150; predicted response = <I>y</I><SUB><SMALL>i</SMALL></SUB> - <IMG SRC="images/14-13i.jpg"></IMGI>
</DL>
<P>and
</P>
<DL>
<DL>
<DD>Variance of errors without regression = <IMG SRC="images/14-14i.jpg"></IMGI>
<BR><P ALIGN="CENTER"><IMG SRC="images/14-17d.jpg"></P>
</IMGD>
<DL>
<DD>= variance of <I>y</I>
</DL>
</DL>
</DL>
<P>The SSE without regression would be
</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-18d.jpg"></P>
</IMGD></P>
<P>This quantity is called the <B>total sum of squares</B> (<B>SST</B>). Although it is different from the variance of <I>y</I> (since we have not divided by <I>n</I> &#150;1), it is a measure of <I>y</I>&#146;s variability and is called the <B>variation</B> of <I>y</I>.</P>
<P>The SST can be computed as follows:</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-19d.jpg"></P>
</IMGD></P>
<P>where <B>SSY</B> is the sum of squares of y (or &#962;<I>y</I><SUP><SMALL>2</SMALL></SUP>) and <B>SS0</B> is the sum of squares of <IMG SRC="images/14-15i.jpg"></IMGI> and is equal to <IMG SRC="images/14-16i.jpg"></IMGI>. The difference between SST and SSE is the sum of squares explained by the regression. It is called <B>SSR:</B></P>
<P ALIGN="CENTER">SSR = SST &#150; SSE</P>
<P>or</P>
<P ALIGN="CENTER">SST = SSR &#43; SSE</P>
<P>Thus, the total variation SST can be divided into two parts. The SSE indicates the variation that was not explained by the regression, while SSR indicates the variation that was explained by the regression. The fraction of the variation that is explained determines the goodness of the regression and is called the <B>coefficient of determination</B>, <I>R</I><SUP><SMALL>2</SMALL></SUP>:</P>
<P ALIGN="CENTER">Coefficient of determination = <I>R</I><SUP><SMALL>2</SMALL></SUP> =<IMG SRC="images/14-17i.jpg"></IMGI></P>
<P>The goodness of a regression is measured by <I>R</I><SUP><SMALL>2</SMALL></SUP>. The higher the value of <I>R</I><SUP><SMALL>2</SMALL></SUP>, the better the regression. If the regression model is perfect in the sense that all observed values are equal to those predicted by the model, that is, all errors are zero, SSE is zero and the coefficient of determination is 1. On the other hand, if the regression model is so bad that it does not reduce the error variance at all, SSE is equal to SST and the coefficient of determination is zero.</P>
<P>The coefficient of determination is denoted by <I>R</I><SUP><SMALL>2</SMALL></SUP> because it is also the square of the sample correlation <I>R<SUB><SMALL>xy</SMALL></SUB></I> between the two variables:</P>
<P ALIGN="CENTER">Sample correlation(<I>x</I>, <I>y</I>) = <I>R</I><SUB><SMALL>xy</SMALL></SUB> = <IMG SRC="images/14-18i.jpg"></IMGI></P>
<P ALIGN="CENTER">Coefficient of determination = {correlation coefficient(<I>x</I>, <I>y</I>)}<SUP><SMALL>2</SMALL></SUP></P>
<P>In computing <I>R</I><SUP><SMALL>2</SMALL></SUP>, it is helpful to compute SSE using the following shortcut formula:</P>
<P ALIGN="CENTER">SSE = &#962;<I>y</I><SUP><SMALL>2</SMALL></SUP> &#150; <I>b</I><SUB><SMALL>0</SMALL></SUB>&#962;<I>y</I> &#150; <I>b</I><SUB><SMALL>1</SMALL></SUB>&#962;<I>x</I><I>y</I></P>
<BLOCKQUOTE><P><B>Example 14.2</B> For the disk I/O-CPU time data of Example 14.1 the coefficient of determination can be computed as follows:</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-20d.jpg"></P>
</IMGD></P>
<P>Thus, the regression explains 97% of CPU time&#146;s variation.</P>
</BLOCKQUOTE><H3><A NAME="Heading5"></A><FONT COLOR="#000077">14.4 STANDARD DEVIATION OF ERRORS</FONT></H3>
<P>To compute the variance of errors, we need to divide the sum of squared errors (SSE) by its degrees of freedom, which are <I>n</I> &#150; 2, since errors are obtained after calculating two regression parameters from the data. Thus, the variance estimate is</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-21d.jpg"></P>
</IMGD></P>
<P>The quantity SSE/(<I>n</I>&#150;2) is called the <I>mean squared error</I> (<B>MSE</B>). The standard deviation of errors is simply a square root of the MSE.</P>
<P>At this point it is interesting to point out the degrees of freedom for other sums of squares as well. The SSY has <I>n</I> degrees of freedom since it is obtained from <I>n</I> independent observations without estimating any parameters. The SS0 has just one degree of freedom since it can be computed simply from <IMG SRC="images/14-19i.jpg"></IMGI>. The SST has <I>n</I>&#150;1 degrees of freedom since one parameter <IMG SRC="images/14-20i.jpg"></IMGI> must be calculated from the data before SST can be computed. The SSR, which is the difference between SST and SSE, has the remaining one degree of freedom. Thus, various sums and their associated degrees of freedom are as follows:</P>
<P ALIGN="CENTER">SST = SSY &#150; SS0 = SSR &#43; SSE</P>
<P ALIGN="CENTER"><I>n</I> &#150; 1 = <I>n</I> &#150; 1 = 1 &#43; (<I>n</I> &#150; 2)</P>
<P>Notice that the degrees of freedom add just the way the sums of squares do. This is an important property that we will use frequently in developing other statistical models.</P>
<BLOCKQUOTE><P><B>Example 14.3</B> For the disk I/O-CPU data of Example 14.1, various sums of squares have already been computed in Example 14.2 The degrees of freedoms of the sums are</P>
<CENTER>
<TABLE WIDTH="80%"><TD VALIGN="TOP" ALIGN="LEFT" WIDTH="30%">Sums of squares:
<TD VALIGN="TOP" ALIGN="LEFT">SST = SSY &#150; SSO=SSR &#43; SSE
<TR>
<TD VALIGN="TOP" ALIGN="LEFT">
<TD VALIGN="TOP" ALIGN="LEFT">205.71 = 828 &#150; 622.29 = 199.84 &#43; 5.87
<TR>
<TD VALIGN="TOP" ALIGN="LEFT">Degrees of freedom:
<TD VALIGN="TOP" ALIGN="LEFT">6 = 7 &#150; 1 = 1 &#43; 5
</TABLE>
</CENTER>
<P>The mean squared error is
</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-22d.jpg"></P>
</IMGD></P>
<P>The standard deviation of errors is</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-23d.jpg"></P>
</P>
</BLOCKQUOTE><H3><A NAME="Heading6"></A><FONT COLOR="#000077">14.5 CONFIDENCE INTERVALS FOR REGRESSION PARAMETERS</FONT></H3>
<P>The regression coefficients <I>b</I><SUB><SMALL>0</SMALL></SUB> and <I>b</I><SUB><SMALL>1</SMALL></SUB> are estimates from a single sample of size <I>n</I>. Using another sample, the estimates may be different. Thus, the coefficients are random in the same manner as the sample mean or any other parameter computed from a sample. Using a single sample, only probabilistic statements can be made about true parameters <I>&#946;</I><SUB><SMALL>0</SMALL></SUB> and <I>&#946;</I><SUB><SMALL>1</SMALL></SUB> of the population. That is, the true model is</P>
<P ALIGN="CENTER"><I>y</I> = <I>&#946;</I><SUB><SMALL>0</SMALL></SUB> &#43; <I>&#946;</I><SUB><SMALL>1</SMALL></SUB><I>x</I></P>
<P>and the computed coefficients <I>b</I><SUB><SMALL>0</SMALL></SUB> and <I>b</I><SUB><SMALL>1</SMALL></SUB> are &#147;statistics&#148; corresponding to the parameters <I>&#946;</I><SUB><SMALL>0</SMALL></SUB> and <I>&#946;</I><SUB><SMALL>1</SMALL></SUB>, respectively.</P>
<P>The values of <I>b</I><SUB><SMALL>0</SMALL></SUB> and <I>b</I><SUB><SMALL>1</SMALL></SUB> obtained from Equations (14.2) and (14.1) are their mean values. Their standard deviations can be obtained from that of the error as follows:</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-24d.jpg"></P>
</IMGD></P>
<P>Here, <I>s</I><SUB><SMALL><I>e</I></SMALL></SUB> is the standard deviation of errors and <IMG SRC="images/14-21i.jpg"></IMGI> is the sample mean of <I>x</I>. The 100(1 &#150; <I>&#945;</I>)% confidence intervals for <I>b</I><SUB><SMALL>0</SMALL></SUB> and <I>b</I><SUB><SMALL>1</SMALL></SUB> can be be computed using <I>t</I><SUB><SMALL>[1 &#150; &#945;/2;<I>n</I> &#150; 2]</SMALL></SUB>&#151; the 1 &#150; <I>&#945;</I>/2 quantile of a <I>t</I> variate with <I>n</I> &#150; 2 degrees of freedom. The confidence intervals are</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-25d.jpg"></P>
</P>
<P>and</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-26d.jpg"></P>
</P>
<P>If a confidence interval includes zero, then the regression parameter cannot be considered different from zero at the 100(1&#150;<I>&#945;</I>)% confidence level.</P><P><BR></P>
<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="14-01.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="14-03.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>

<hr width="90%" size="1" noshade>
<div align="center">
<font face="Verdana,sans-serif" size="1">Copyright &copy; <a href="/reference/wiley00001.html">John Wiley &amp; Sons, Inc.</a></font>
</div>
<!-- all of the reference materials (books) have the footer and subfoot reveresed -->
<!-- reference_subfoot = footer -->
<!-- reference_footer = subfoot -->

</BODY>
</HTML>

<!-- END FOOTER -->

