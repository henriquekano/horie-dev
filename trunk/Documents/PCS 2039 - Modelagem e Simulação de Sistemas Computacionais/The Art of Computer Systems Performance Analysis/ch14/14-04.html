<HTML>
<HEAD>
<META name=vsisbn content="0471503363">
<META name=vstitle content="Art of Computer Systems Performance Analysis Techniques For Experimental Design Measurements Simulation And Modeling">
<META name=vsauthor content="Raj Jain">
<META name=vsimprint content="Wiley Computer Publishing">
<META name=vspublisher content="John Wiley & Sons, Inc.">
<META name=vspubdate content="05/01/91">
<META name=vscategory content="Web and Software Development: Software Engineering: Simulation and Modeling">







<TITLE>The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling:Simple Linear Regression Models</TITLE>

<!-- HEADER -->

<STYLE type="text/css"> 
 <!--
 A:hover  {
 	color : Red;
 }
 -->
</STYLE>

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">

<!--ISBN=0471503363//-->
<!--TITLE=The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling//-->
<!--AUTHOR=Raj Jain//-->
<!--PUBLISHER=Wiley Computer Publishing//-->
<!--CHAPTER=14//-->
<!--PAGES=232-238//-->
<!--UNASSIGNED1//-->
<!--UNASSIGNED2//-->

<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="14-03.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="14-05.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>
<P><BR></P>
<H3><A NAME="Heading7"></A><FONT COLOR="#000077">14.6 CONFIDENCE INTERVALS FOR PREDICTIONS</FONT></H3>
<P>The purpose of developing regression usually is to predict the value of the response variable for those values of predictor variables that have not been measured. Given the regression equation, it is easy to predict the response <IMG SRC="images/14-23i.jpg"></IMGI> for any given value of predictor variable <I>x</I><SUB><SMALL><I>p</I></SMALL></SUB>:</P>
<TABLE WIDTH="100%"><TR>
<TD WIDTH="90%" ALIGN="CENTER" VALIGN="TOP"><IMG SRC="images/14-24i.jpg"></IMGI>
<TD WIDTH="10%" ALIGN="LEFT" VALIGN="TOP">(14.3)
</TABLE>
<P>This formula gives only the mean value of the predicted response based upon the sample. Like most of the other computations based on the sample, it is necessary to specify a confidence interval for this predicted mean. The formula for the standard deviation of the mean of a future sample of <I>m</I> observations is</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-30d.jpg"></P>
</IMGD></P>
<P>There are two special cases of this formula that are of interest. One case is for <I>m</I> = 1. This gives the standard deviation of a single future observation:</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-31d.jpg"></P>
</IMGD></P>
<P><A NAME="Fig5"></A><A HREF="javascript:displayWindow('images/14-05.jpg',500,348 )"><IMG SRC="images/14-05t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/14-05.jpg',500,348)"><FONT COLOR="#000077"><B>FIGURE 14.5</B></FONT></A>&nbsp;&nbsp;Confidence intervals for predictions from regression models.</P>
<P>The second case is for <I>m</I> = &#8734;. This gives the standard deviation of the mean of a large number of future observations at <I>x</I><SUB><SMALL><I>p</I></SMALL></SUB>:</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-32d.jpg"></P>
</IMGD></P>
<P>Notice that the standard deviation for the mean of an infinite future sample is lower than that of finite samples since in the latter case the error associated with the future observations should also be accounted for.</P>
<P>In all cases discussed above, a 100(1 &#150; &#945;)% confidence interval for the mean can be constructed using a <I>t</I> quantile read at <I>n</I> &#150; 2 degrees of freedom.</P>
<P>It is interesting to note from the above expressions that the standard deviation of the prediction is minimal at the center of the measured range (at <IMG SRC="images/14-25i.jpg"></IMGI>) and it increases as we move away from the center. Since the goodness of any statistical prediction is indicated by its standard deviation, the goodness of the prediction decreases as we move away from the center. This is shown schematically in Figure 14.5. In particular, if we try to predict far beyond the measured range, the variance of the prediction will be large, the confidence interval will be wide, and the accuracy of the prediction will be low.</P>
<BLOCKQUOTE><P><B>Example 14.5</B> Using the disk I/O and CPU time data of Example 14.1, let us estimate the CPU time for a program with 100 disk I/O&#146;s.</P>
<P>In this case, we have already seen that the regression equation is</P>
<P ALIGN="CENTER">CPU time = &#150;0.0083 &#43; 0.2438(number of disk I/O&#146;s)</P>
<P>Therefore, for a program with 100 disk I/O&#146;s, the mean CPU time is</P>
<P ALIGN="CENTER">CPU time = &#150;0.0083 &#43; 0.2438(100) = 24.3674</P>
<P ALIGN="CENTER">Standard deviation of effors <I>s</I><SUB><SMALL><I>e</I></SMALL></SUB> = 1.0834</P>
<P>The standard deviation of the predicted mean of a large number of observations is</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-33d.jpg"></P>
</IMGD></P>
<P>From Table A.4 in the Appendix, the 0.95-quantile of the <I>t</I>-variate with five degrees of freedom is 2.015.</P>
<P>90% confidence interval for predicted mean = <IMG SRC="images/14-26i.jpg"></IMGI></P>
<P>Thus, we can say with 90% confidence that the mean CPU time for a program making 100 disk I/O&#146;s will be between 21.9 and 26.9 milliseconds. This prediction assumes that we will take a large number observations for such programs and then take a mean.</P>
<P>To set bounds on the CPU time of a single future program with 100 disk I/O&#146;s, the computation is as follows:</P>
<P><P ALIGN="CENTER"><IMG SRC="images/14-34d.jpg"></P>
</IMGD></P>
<P>90% confidence interval for single prediction = <IMG SRC="images/14-27i.jpg"></IMGI></P>
<P>Notice that the confidence interval for the single prediction is wider than that for the mean of a large number of observations.</P>
</BLOCKQUOTE><H3><A NAME="Heading8"></A><FONT COLOR="#000077">14.7 VISUAL TESTS FOR VERIFYING THE REGRESSION ASSUMPTIONS</FONT></H3>
<P>In deriving the expressions for regression parameters, we made the following assumptions:
</P>
<DL>
<DD><B>1.</B>&nbsp;&nbsp;The true relationship between the response variable <I>y</I> and the predictor variable <I>x</I> is linear.
<DD><B>2.</B>&nbsp;&nbsp;The predictor variable <I>x</I> is nonstochastic and it is measured without any error.
<DD><B>3.</B>&nbsp;&nbsp;The model errors are statistically independent.
<DD><B>4.</B>&nbsp;&nbsp;The errors are normally distributed with zero mean and a constant standard deviation.
</DL>
<P>If any of the assumptions are violated, the conclusions based on the regression model would be misleading. In this section, we describe a number of visual techniques to verify that these assumptions hold. Unlike statistical tests, all visual techniques are approximate. However, we have found them useful for two reasons. First, they are easier to explain to decision makers who may not understand statistical tests. Second, they often provide more information than a simple &#147;pass-fail&#148; type answer obtained from a test. Often, using a visual test, one can also find the cause of the problem.
</P>
<P>The assumptions, which can be visually tested, and the corresponding tests are as follows:</P>
<DL>
<DD><B>1.</B>&nbsp;&nbsp;<I>Linear Relationship</I>: Prepare a scatter plot of <I>y</I> versus <I>x</I>. Any nonlinear relationship can be easily seen from this plot. Figure 14.6 shows a number of hypothetical possibilities. In case (a), the relationship appears to be linear and the linear model can be used. In case (b), there seems to be two different regions of operation, and the relationship is linear in both regions; thus, two separate linear regressions should be used. In case (c), there is one point, which is quite different from the remaining points. This may be due to some measurement error. The values must be rechecked, and if possible, measurements should also be made at other intermediate values. In case (d), the points appear to be related but the relationship is nonlinear; a curvilinear regression (discussed in Section 15.3) should be used in place of a linear regression.
<DD><B>2.</B>&nbsp;&nbsp;<I>Independent Errors</I>: After the regression, compute errors and prepare a scatter plot of <I>&#8712;</I><SUB><SMALL><I>i</I></SMALL></SUB> versus the predicted response <IMG SRC="images/14-28i.jpg"></IMGI>. Any visible trends in the scatter plot would indicate a dependence of errors on the predictor variable. Figure 14.7 shows three hypothetical plots of error versus predicted response. In case (a), there is no visible trend or clustering of points, and therefore, the errors appear to be independent. In case (b), we see that the errors increase with increasing response. In case (c), the trend is nonlinear. Any such trend is indicative of an inappropriate model. It is quite possible that a linear model is not appropriate for this case, and either the curvilinear model discussed in Section 15.4 or one of the transformations discussed in Section 15.4 should be tried.
<P><A NAME="Fig6"></A><A HREF="javascript:displayWindow('images/14-06.jpg',500,416 )"><IMG SRC="images/14-06t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/14-06.jpg',500,416)"><FONT COLOR="#000077"><B>FIGURE 14.6</B></FONT></A>&nbsp;&nbsp;Possible patterns of scatter diagrams.</P>
<P><A NAME="Fig7"></A><A HREF="javascript:displayWindow('images/14-07.jpg',500,382 )"><IMG SRC="images/14-07t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/14-07.jpg',500,382)"><FONT COLOR="#000077"><B>FIGURE 14.7</B></FONT></A>&nbsp;&nbsp;Possible patterns of residual versus predicted response graphs.</P>
<BR>You may also want to plot the residuals as a function of the experiment number where the experiments are numbered in the order they are conducted. As shown in Figure 14.8, any trend up or down in such a plot would indicate the presence of other factors, environmental conditions (temperature, humidity, and so on), or side effects (incorrect initializations) that varied from one experiment to the next and affected the response. The cause of such trends should be identified. If additional factors are found to affect the response, they also should be included in the analysis.
<P><A NAME="Fig8"></A><A HREF="javascript:displayWindow('images/14-08.jpg',500,205 )"><IMG SRC="images/14-08t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/14-08.jpg',500,205)"><FONT COLOR="#000077"><B>FIGURE 14.8</B></FONT></A>&nbsp;&nbsp;A trend in the residual versus experiment number may indicate side effects or incorrect initializations.</P>
<P><A NAME="Fig9"></A><A HREF="javascript:displayWindow('images/14-09.jpg',500,182 )"><IMG SRC="images/14-09t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/14-09.jpg',500,182)"><FONT COLOR="#000077"><B>FIGURE 14.9</B></FONT></A>&nbsp;&nbsp;The normal quantile-quantile plots of the residuals should be a straight line.</P>
<BR>We must point out that there is no foolproof test for independence. All tests for independence simply try to find dependence of one kind or the other. Thus, passing one test proves that the test was unable to find any dependence. This does not mean that another test will also not find any dependence. In other words, dependence can be proven in practice but independence cannot.
<DD><B>3.</B>&nbsp;&nbsp;<I>Normally Distributed Errors</I>: Prepare a normal quantile-quantile plot of errors. If the plot is approximately linear, the assumption is satisfied. Figure 14.9 shows two hypothetical examples of such plots. In case (a), the plot is approximately linear, and so the assumption of normally distributed errors is valid. In case (b), there is no visible linearity and the errors do not seem to be normally distributed.
<DD><B>4.</B>&nbsp;&nbsp;<I>Constant Standard Deviation of Errors</I>: This is also known as <B>homoscedasticity</B>. To verify it, observe the scatter plot of errors versus predicted response prepared for the independence test. If the spread in one part of the graph seems significantly different than that in other parts, then the assumption of constant variance is not valid.
<BR>Figure 14.10 shows two hypothetical examples. In case (a), the spread is homogeneous. In case (b), the spread appears to be increasing as the predicted response increases. This implies that the distribution of the errors still depends on the predictor variables. The regression model does not fully incorporate the effect of predictors. The linear model is not a good model in this case. A curvilinear regression should be tried instead. A transformation of the response, for example, using log(y) instead of y, may also help eliminate the problem. The transformations are discussed later in Section 15.4.
</DL>
<P><A NAME="Fig10"></A><A HREF="javascript:displayWindow('images/14-10.jpg',500,188 )"><IMG SRC="images/14-10t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/14-10.jpg',500,188)"><FONT COLOR="#000077"><B>FIGURE 14.10</B></FONT></A>&nbsp;&nbsp;A trend in the spread of residuals as a function of the predicted response indicates a need for transformation or a nonlinear regression.</P>
<P><A NAME="Fig11"></A><A HREF="javascript:displayWindow('images/14-11.jpg',500,402 )"><IMG SRC="images/14-11t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/14-11.jpg',500,402)"><FONT COLOR="#000077"><B>FIGURE 14.11</B></FONT></A>&nbsp;&nbsp;Graph of residual versus predicted response for the disk I/O and CPU time data.<P><BR></P>
<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="14-03.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="14-05.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>

<hr width="90%" size="1" noshade>
<div align="center">
<font face="Verdana,sans-serif" size="1">Copyright &copy; <a href="/reference/wiley00001.html">John Wiley &amp; Sons, Inc.</a></font>
</div>
<!-- all of the reference materials (books) have the footer and subfoot reveresed -->
<!-- reference_subfoot = footer -->
<!-- reference_footer = subfoot -->

</BODY>
</HTML>

<!-- END FOOTER -->

