<HTML>
<HEAD>
<META name=vsisbn content="0471503363">
<META name=vstitle content="Art of Computer Systems Performance Analysis Techniques For Experimental Design Measurements Simulation And Modeling">
<META name=vsauthor content="Raj Jain">
<META name=vsimprint content="Wiley Computer Publishing">
<META name=vspublisher content="John Wiley & Sons, Inc.">
<META name=vspubdate content="05/01/91">
<META name=vscategory content="Web and Software Development: Software Engineering: Simulation and Modeling">







<TITLE>The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling:Workload Characterization Techniques</TITLE>

<!-- HEADER -->

<STYLE type="text/css"> 
 <!--
 A:hover  {
 	color : Red;
 }
 -->
</STYLE>

<META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">

<!--ISBN=0471503363//-->
<!--TITLE=The Art of Computer Systems Performance Analysis: Techniques for Experimental Design, Measurement, Simulation, and Modeling//-->
<!--AUTHOR=Raj Jain//-->
<!--PUBLISHER=Wiley Computer Publishing//-->
<!--CHAPTER=06//-->
<!--PAGES=085-088//-->
<!--UNASSIGNED1//-->
<!--UNASSIGNED2//-->

<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="06-06.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="06-08.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>
<P><BR></P>
<H4 ALIGN="LEFT"><A NAME="Heading14"></A><FONT COLOR="#000077">6.8.5 Data Scaling</FONT></H4>
<P>The final results of clustering depend heavily upon relative values and ranges of different parameters. It is therefore generally recommended that the parameter values be scaled so that their relative values and ranges are approximately equal. The four commonly used scaling techniques are as follows:
</P>
<DL>
<DD><B>1.</B>&nbsp;&nbsp;<I>Normalize to Zero Mean and Unit Variance</I>: Let {<I>x</I><SUB><SMALL>1</SMALL></SUB><SUB><SMALL><I>k</I></SMALL></SUB>,<I>x</I><SUB><SMALL>2</SMALL></SUB><SUB><SMALL><I>k</I></SMALL></SUB>,...,<I>x</I><SUB><SMALL><I>mk</I></SMALL></SUB>} be the measured values of the <I>k</I>th parameter. The scaled value <I>x</I>&#146;<SUB><SMALL><I>ik</I></SMALL></SUB> corresponding to <I>x</I><SUB><SMALL><I>ik</I></SMALL></SUB> is given by
<BR><P ALIGN="CENTER"><IMG SRC="images/06-15d.jpg"></P>
</IMGD>
<BR>Here <IMG SRC="images/06-02i.jpg"></IMGI> and <I>s</I><SUB><SMALL><I>k</I></SMALL></SUB> are the measured mean and standard deviation of the <I>k</I>th parameter, respectively.
<DD><B>2.</B>&nbsp;&nbsp;<I>Weights</I>:
<P ALIGN="CENTER"><I>x'<SUB><SMALL>ik</SMALL></SUB> = w<SUB><SMALL>k</SMALL></SUB>x<SUB><SMALL>ik</SMALL></SUB></I>
</DL>
<DL>
<DD><B></B>&nbsp;&nbsp;
<BR>The weight <I>w</I><SUB><SMALL><I>k</I></SMALL></SUB> may be assigned depending upon the relative importance of the parameter or is inversely proportional to the standard deviation of the parameter values.
<DD><B>3.</B>&nbsp;&nbsp;<I>Range Normalization</I>: The range is changed from [<I>x</I><SUB><SMALL>min,<I>k</I></SMALL></SUB>, <I>x</I><SUB><SMALL>max,<I>k</I></SMALL></SUB>] to [0, 1]. The scaling formula is
<BR><P ALIGN="CENTER"><IMG SRC="images/06-16d.jpg"></P>
</IMGD>
<BR>Here <I>x</I><SUB><SMALL>min,<I>k</I></SMALL></SUB> and <I>x</I><SUB><SMALL>max,<I>k</I></SMALL></SUB> are the minimum and maximum values, respectively, of the <I>k</I>th parameter. The main problem with this approach is that a few outliers can drastically impact <I>x</I><SUB><SMALL>min,<I>k</I></SMALL></SUB> and <I>x</I><SUB><SMALL>max,<I>k</I></SMALL></SUB> and hence the final outcome of clustering. This disadvantage is overcome by using percentiles rather than strict minimum and maximum, as discussed next.
<DD><B>4.</B>&nbsp;&nbsp;<I>Percentile Normalization</I>: The data is scaled so that 95% of the values fall between 0 and 1:
<BR><P ALIGN="CENTER"><IMG SRC="images/06-17d.jpg"></P>
</IMGD>
<BR>Here <I>x</I><SUB><SMALL>2.5,<I>k</I></SMALL></SUB> and <I>x</I><SUB><SMALL>97.5,<I>k</I></SMALL></SUB> are the 2.5- and 97.5-percentiles, respectively, of the <I>k</I>th parameter.
</DL>
<H4 ALIGN="LEFT"><A NAME="Heading15"></A><FONT COLOR="#000077">6.8.6 Distance Metric</FONT></H4>
<P>Clustering analysis basically consists of mapping each component into an n-dimensional space and identifying components that are close to each other. Here <I>n</I> is the number of parameters. The closeness between two components is measured by defining a distance measure. Three methods that have been used are as follows:</P>
<DL>
<DD><B>1.</B>&nbsp;&nbsp;<I>Euclidean Distance</I>: The distance <I>d</I> between two components {<I>x</I><SUB><SMALL><I>i</I>1</SMALL></SUB>,<I>x</I><SUB><SMALL><I>i</I>2</SMALL></SUB>,...,<I>x</I><SUB><SMALL><I>in</I></SMALL></SUB>} and {<I>x</I><SUB><SMALL><I>j</I>1</SMALL></SUB>,<I>x</I><SUB><SMALL><I>j</I>2</SMALL></SUB>,...,<I>x</I><SUB><SMALL><I>jn</I></SMALL></SUB>} is defined as
<BR><P ALIGN="CENTER"><IMG SRC="images/06-18d.jpg"></P>
</IMGD>
<DD><B>2.</B>&nbsp;&nbsp;<I>Weighted Euclidean Distance</I>:
<BR><P ALIGN="CENTER"><IMG SRC="images/06-19d.jpg"></P>
</IMGD>
<BR>Here <I>a</I><SUB><SMALL><I>k</I></SMALL></SUB>, <I>k</I> = 1, 2,..., <I>n</I>, are suitably chosen weights for the <I>n</I> parameters.
<DD><B>3.</B>&nbsp;&nbsp;<I>Chi-Square Distance</I>:
<BR><P ALIGN="CENTER"><IMG SRC="images/06-20d.jpg"></P>
</IMGD>
<BR>The Euclidean distance is the most commonly used distance metric. The weighted Euclidean is used if the parameters have not been scaled or if the parameters have significantly different levels of importance.
<BR>The chi-square distance is generally used in distribution fitting. In using this measure, it is important that the values <I>x</I><SUB><SMALL><I>.k</I></SMALL></SUB>&#146;s be close to each other, that is, they have been normalized; otherwise, parameters with low values of <I>x</I><SUB><SMALL>.k</SMALL></SUB> get higher weights.
</DL>
<H4 ALIGN="LEFT"><A NAME="Heading16"></A><FONT COLOR="#000077">6.8.7 Clustering Techniques</FONT></H4>
<P>The basic aim of clustering is to partition the components into groups so the members of a group are as similar as possible and different groups are as dissimilar as possible. Statistically, this implies that the intragroup variance should be as small as possible and intergroup variance should be as large as possible. Fortunately, these two goals are redundant in the sense that achieving either one is sufficient. This is because
</P>
<P ALIGN="CENTER">Total variance = intragroup variance &#43; intergroup variance</P>
<P>Since the total variance is constant, minimizing intragroup variance automatically maximizes intergroup variance.</P>
<P>A number of clustering techniques have been described in the literature. These techniques fall into two classes: hierarchical and nonhierarchical. In nonhierarchical approaches, one starts with an arbitrary set of <I>k</I> clusters, and the members of the clusters are moved until the intragroup variance is minimum. There are two kinds of hierarchical approaches: agglomerative and divisive. In the agglomerative hierarchical approach, given <I>n</I> components, one starts with <I>n</I> clusters (each cluster having one component). Then neighboring clusters are merged successively until the desired number of clusters is obtained. In the divisive hierarchical approach, on the other hand, one starts with one cluster (of <I>n</I> components) and then divides the cluster successively into two, three, and so on, until the desired number of clusters is obtained. A popular technique known as minimum spanning tree method is described next.</P>
<H4 ALIGN="LEFT"><A NAME="Heading17"></A><FONT COLOR="#000077">6.8.8 Minimum Spanning Tree Method</FONT></H4>
<P>This is an agglomerative hierarchical clustering technique, which starts with <I>n</I> clusters of one component each and successively joins the nearest clusters:</P>
<DL>
<DD><B>1.</B>&nbsp;&nbsp;Start with <I>k</I> = <I>n</I> clusters.
<DD><B>2.</B>&nbsp;&nbsp;Find the centroid of the <I>i</I>th cluster, <I>i</I> = 1, 2, ..., <I>k</I>. The centroid has parameter values equal to the average of all points in the cluster.
<DD><B>3.</B>&nbsp;&nbsp;Compute the intercluster distance matrix. Its (<I>i</I>, <I>j</I>)th element is the distance between the centroids of cluster <I>i</I> and <I>j</I>. Any distance measure described in Section 6.8.6 can be used.
<DD><B>4.</B>&nbsp;&nbsp;Find the smallest nonzero element of the distance matrix. Let <I>d</I><SUB><SMALL><I>lm</I></SMALL></SUB>, the distance between clusters <I>l</I> and <I>m</I>, be the smallest. Merge clusters <I>l</I> and <I>m</I>. Also merge any other cluster pairs that have the same distance.
<DD><B>5.</B>&nbsp;&nbsp;Repeat steps 2 to 4 until all components are part of one cluster.
</DL>
<P>Example 6.3 illustrates these steps.
</P>
<P><B>Example 6.3</B> Consider a workload with five components and two parameters. The CPU time and the number of disk I/O&#146;s were measured for five programs. Tbe parameter values after scaling are as shown in Table 6.6.</P>
<TABLE ALIGN="CENTER"><TH CAPTION COLSPAN="3">TABLE 6.6 Data for Clustering Example 6.3
<TR>
<TD COLSPAN="3"><HR>
<TR>
<TD VALIGN="TOP" ALIGN="CENTER">Program
<TD VALIGN="TOP" ALIGN="CENTER">CPU Time
<TD VALIGN="TOP" ALIGN="CENTER">Disk I/O
<TR>
<TD COLSPAN="3"><HR>
<TR>
<TD VALIGN="TOP" ALIGN="CENTER">A
<TD VALIGN="TOP" ALIGN="CENTER">2
<TD VALIGN="TOP" ALIGN="CENTER">4
<TR>
<TD VALIGN="TOP" ALIGN="CENTER">B
<TD VALIGN="TOP" ALIGN="CENTER">3
<TD VALIGN="TOP" ALIGN="CENTER">5
<TR>
<TD VALIGN="TOP" ALIGN="CENTER">C
<TD VALIGN="TOP" ALIGN="CENTER">1
<TD VALIGN="TOP" ALIGN="CENTER">6
<TR>
<TD VALIGN="TOP" ALIGN="CENTER">D
<TD VALIGN="TOP" ALIGN="CENTER">4
<TD VALIGN="TOP" ALIGN="CENTER">3
<TR>
<TD VALIGN="TOP" ALIGN="CENTER">E
<TD VALIGN="TOP" ALIGN="CENTER">5
<TD VALIGN="TOP" ALIGN="CENTER">2
<TR>
<TD COLSPAN="3"><HR>
</TABLE>
<DL>
<DD>Step 1: Consider five clusters with <I>i</I>th cluster consisting solely of the <I>i</I>th program.
<DD>Step 2: The centroids are {2,4}, {3,5}, {1,6}, {4,3}, and {5,2}. These are shown by the five points in Figure 6.6.
<DD>Step 3: Using the Euclidean distance measure, the distance matrix is
</DL>
<P><P ALIGN="CENTER"><IMG SRC="images/06-21d.jpg"></P>
</IMGD></P>
<P><A NAME="Fig6"></A><A HREF="javascript:displayWindow('images/06-06.jpg',500,316 )"><IMG SRC="images/06-06t.jpg"></A>
<BR><A HREF="javascript:displayWindow('images/06-06.jpg',500,316)"><FONT COLOR="#000077"><B>FIGURE 6.6</B></FONT></A>&nbsp;&nbsp;Clustering example.<P><BR></P>
<CENTER>
<TABLE BORDER>
<TR>
<TD><A HREF="06-06.html">Previous</A></TD>
<TD><A HREF="../ewtoc.html">Table of Contents</A></TD>
<TD><A HREF="06-08.html">Next</A></TD>
</TR>
</TABLE>
</CENTER>

<hr width="90%" size="1" noshade>
<div align="center">
<font face="Verdana,sans-serif" size="1">Copyright &copy; <a href="/reference/wiley00001.html">John Wiley &amp; Sons, Inc.</a></font>
</div>
<!-- all of the reference materials (books) have the footer and subfoot reveresed -->
<!-- reference_subfoot = footer -->
<!-- reference_footer = subfoot -->

</BODY>
</HTML>

<!-- END FOOTER -->

